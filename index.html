<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Lightning Nowcasting</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Georgia', 'Times New Roman', serif;
            line-height: 1.6;
            color: #333;
            background-color: #fff;
            font-size: 16px;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 40px 20px;
        }

        /* Header */
        .header {
            text-align: center;
            margin-bottom: 50px;
            border-bottom: 2px solid #eee;
            padding-bottom: 30px;
        }

        .header h1 {
            font-size: 2.5em;
            font-weight: 700;
            color: #2c3e50;
            margin-bottom: 10px;
            letter-spacing: -1px;
        }

        .header .subtitle {
            font-size: 1.2em;
            color: #7f8c8d;
            font-style: italic;
            margin-bottom: 20px;
        }

        .header .author {
            font-size: 1.1em;
            color: #34495e;
        }

        /* Section Headers */
        h2 {
            font-size: 1.8em;
            color: #2c3e50;
            margin: 40px 0 20px 0;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }

        h3 {
            font-size: 1.4em;
            color: #34495e;
            margin: 30px 0 15px 0;
        }

        h4 {
            font-size: 1.2em;
            color: #34495e;
            margin: 25px 0 10px 0;
        }

        /* Paragraphs */
        p {
            margin-bottom: 20px;
            text-align: justify;
        }

        /* Lists */
        ul {
            margin: 20px 0;
            padding-left: 30px;
        }

        li {
            margin-bottom: 8px;
        }

        /* Images */
        .figure {
            margin: 30px 0;
            text-align: center;
        }

        .figure img {
            max-width: 100%;
            height: auto;
            border: 1px solid #ddd;
            border-radius: 8px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
        }

        .figure .caption {
            font-style: italic;
            color: #7f8c8d;
            margin-top: 10px;
            font-size: 0.95em;
            text-align: center;
            max-width: 800px;
            margin-left: auto;
            margin-right: auto;
        }

        /* Key findings box */
        .findings-box {
            background-color: #f8f9fa;
            border-left: 4px solid #3498db;
            padding: 25px;
            margin: 30px 0;
            border-radius: 0 8px 8px 0;
        }

        .findings-box h3 {
            margin-top: 0;
            color: #2c3e50;
        }

        /* Links */
        a {
            color: #3498db;
            text-decoration: none;
        }

        a:hover {
            text-decoration: underline;
        }

        /* Footer */
        .footer {
            margin-top: 60px;
            padding-top: 30px;
            border-top: 2px solid #eee;
            text-align: center;
            color: #7f8c8d;
        }

        .footer .contact {
            margin-top: 20px;
        }

        .footer .contact a {
            margin: 0 15px;
            font-weight: 500;
        }

        /* Responsive design */
        @media (max-width: 768px) {
            .container {
                padding: 20px 15px;
            }

            .header h1 {
                font-size: 2em;
            }

            h2 {
                font-size: 1.5em;
            }

            .figure img {
                border-radius: 4px;
            }
        }

        /* Code/tech stack styling */
        .tech-stack {
            background-color: #f4f4f4;
            padding: 15px;
            border-radius: 6px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
            margin: 20px 0;
        }

        /* Highlight important numbers */
        .stat {
            font-weight: bold;
            color: #e74c3c;
        }

        /* Interactive iframe */
        iframe {
            border: 1px solid #ddd;
            border-radius: 8px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
        }

        .abstract {
            margin: 40px 0;
        }

        .abstract p {
            line-height: 1.7;
        }

        body {
            counter-reset: section;
        }

        h2 {
            counter-reset: subsection;
        }

        h2::before {
            counter-increment: section;
            content: counter(section) ". ";
        }

        h3::before {
            counter-increment: subsection;
            content: counter(section) "." counter(subsection) " ";
        }

        /* Disable numbering for specific headings */
        .no-number::before {
            content: "" !important;
            counter-increment: none !important;
        }

        .figure img.small-img {
            max-width: 60%;
        }

        .figure img.medium-img {
            max-width: 75%;
        }

        .figure img.large-img {
            max-width: 90%;
        }
    </style>
</head>

<body>
    <div class="container">
        <!-- Header -->
        <div class="header">
            <h1>Machine Learning for Radar-Based Lightning Nowcasting</h1>
            <div class="subtitle">A Proof-of-Concept Study using TensorFlow</div>
            <div class="author">Analysis by Harald Thommesen</div>
        </div>

        <div
            style="text-align: center; margin-bottom: 30px; padding: 15px; background-color: #f8f9fa; border-radius: 8px;">
            <strong>Project Links:</strong>
            <a href="https://github.com/haraltho/lightning-nowcasting-deep-learning" target="_blank"
                style="margin: 0 15px;">
                GitHub Repository</a>
            <a href="https://www.linkedin.com/in/h-thommesen/" target="_blank" style="margin: 0 15px;">LinkedIn</a>
        </div>

        <div class="abstract">
            <h2 class="no-number" style="text-align: center; margin-bottom: 15px;">Abstract</h2>
            <p
                style="font-style: italic; text-align: justify; background-color: #f8f9fa; padding: 20px; border-radius: 8px; margin: 30px 0;">
                This project explores whether lightning activity can be predicted directly from polarimetric radar data
                using deep learning. Six years of radar data from the Hurum station near Oslo, combined with
                lightning observations, were processed into a common 3D grid and used to train three deep-learning
                models: a 2D CNN, a ConvLSTM2D, and a ConvLSTM3D. The results show that radar reflectivity alone already
                contains meaningful predictive power. With a nowcasting setup (lead time 0, 30-min window), all models
                achieve similar performance, with CSI values around 0.24-0.25 and recall near 40%. This study
                demonstrates that radar-only ML lightning nowcasting is possible, even with simple models, and provides
                a baseline for future operational usage.
            </p>
        </div>

        <!-- Introduction -->
        <h2>Introduction</h2>
        <p>Lightning is one of the most dangerous and unpredictable weather hazards. Accurate short-term forecasts are
            important for aviation, offshore industry, energy infrastructure, and public safety. If you know that the
            risk for lightning is high in the next few minutes or half hour, you can delay landings or divert planes,
            stop outdoor activities, or protect equipment. These are all decisions that depend on fast and reliable
            nowcasts rather than long-term forecasts. </p>
        <p>Today, most lightning prediction systems depend on numerical weather models that estimate the atmospheric
            conditions that enable lightning. While these approaches work well for day-ahead forecasting, they are not
            really designed to answer the question: <em>What will happen right now, in the next 10-30 minutes?</em>
            Short-term lightning forecasting is still a rather unsolved problem.</p>
        <p>This project explores whether machine learning can fill this gap. Instead of using weather model output, the
            idea is to predict lightning directly from polarimetric radar observations. The radar sweeps the atmosphere
            and generates a 3D snapshot every ten minutes. If a model can learn the spatial and temporal patterns that
            lead to lightning, it could be used as a fast and data-driven warning system. </p>
        <p>A useful analogy to this problem is the use of deep learning models to identify tumours in MRI scans. These
            models don't require any knowledge of human anatomy since they find these patterns on their own. In the same
            way, this project attempts to use tomographic images of the atmosphere to "diagnose" lightning, without
            explicitly modelling the physical processes that lead to these weather events. </p>

        <p>Therefore, the goal of this project is to <strong>test whether lightning prediction using deep learning
                models is feasible, and to establish a proof-of-concept baseline.</strong></p>

        <div class="figure">
            <img src="images/MIT_brain_imaging.jpg" class="medium-img" alt="MRI segmentation example from MIT">
            <div class="caption">
                MRI image example used for machine learning in medical use.<br>
                Image credit: Courtesy of the researchers, MIT News<br>
                Source: <a href="https://news.mit.edu/2018/faster-analysis-of-medical-images-0618" target="_blank">
                    https://news.mit.edu/2018/faster-analysis-of-medical-images-0618</a>
            </div>
        </div>
        <div class="figure">
            <img src="images/atmosphere_tomography.png" class="medium-img" alt="Radar tomography of the atmosphere">
            <div class="caption">
                Example of 3D radar-derived “slices” of the atmosphere used as ML input features.<br>
                (Generated from Hurum radar data, this project.)
            </div>
        </div>

        <h2>Data</h2>
        <p>This project uses two meteorological datasets: <em>lightning observations</em> and <em>polarimetric radar
                measurements</em>.
            The lightning data represent the target variable (or label), while the radar
            volume scans are the atmospheric features the models learn from. Both
            datasets are processed and projected onto a common 3D grid so they can be used together in the
            machine-learning pipeline.</p>

        <h3>Common grid definition</h3>
        <p>Before combining radar and lightning data, both must be transformed onto a common spatial grid. The test grid
            is a
            10x10 grid centered on the Hurum radar station, covers an area of 80x80 km, and has a cell size of 8 km.
            Vertically it spans 0.2-9.7 km with 500 m spacing, thereby covering 20 altitude levels. All downstream
            preprocessing such as interpolation, feature/target pair construction and ML tensors are defined on this
            grid.</p>

        <div class="figure">
            <img src="images/grid_definition.png" class="small-img" alt="Common grid definition">
            <div class="caption">
                Definition of the common test grid centered on the Hurum radar station. It covers an area of 80 by 80
                km with a cell size of 8 km.
            </div>
        </div>



        <h3>Lightning</h3>
        <p>The lightning observations were accessed through MET Norway's <a href="https://frost.met.no/">FROST API</a>,
            which include cloud-to-ground (CG) and intra-cloud (IC) strikes. The former are classic lightning strikes
            where the discharge happens from the cloud down to the ground. The latter are discharges between clouds or
            within a single cloud, typically seen on the onset of thunderstorms. These lightning events are
            triangulated using MET Norway's network of electromagnetic sensors, and each event includes the geographic
            coordinates and its intensity. Each event is projected onto the grid and aggregated into counts per cell per
            10-minute interval. The dataset includes all lightning days from May-August
            2020-2025.
        </p>

        <div class="figure">
            <img src="images/lightning_grid.png" class="small-img" alt="lightning grid">
            <div class="caption">
                Lightning observations aggregated into the 8x8 km grid cells. Blue dots represent intra-cloud strikes
                (IC), red dots represent cloud-to-ground strikes (CG). For each 10-minute interval, the IC, CG, and
                total counts are stored for every grid cell.
            </div>
        </div>


        <h3>Radar</h3>
        <p>The radar data come from MET Norway's polarimetric weather radar at Hurum, located south of Oslo. Every 10
            minutes the radar performs a full volume scan consisting of 12 elevation angles, producing a
            three-dimensional snapshot of the atmosphere in range-azimuth-elevation space.</p>

        <p>Each sweep measures several polarimetric parameters, of which four are used in this project: reflectivity
            (dBZ), differential reflectivity (ZDR), specific differential phase (KDP), and the correlation coefficient
            (RhoHV). Reflectivity measures how strong the radar return is and correlates strongly with precipitation
            intensity. ZDR gives information on the drop shape. Oblate raindrops reflect the vertical and horizontal
            plane of the reflected signal differently, leading to higher ZDR values. KDP responds strongly to rain rate
            and the presence of many liquid drops. RhoHV measures how similar the horizontal and the vertical radar
            signals are, which can be used to identify areas there are mixed phases, such as hail or melting layers.
        </p>

        <div class="figure">
            <img src="images/radar_cone_illustration.png" class="large-img" alt="illustration of radar cone">
            <div class="caption">
                A single radar elevation sweep forms a cone-shaped slice of the atmosphere. Hurum performs 12 such
                sweeps per volume scan (one shown here). These
                measurements need to be mapped onto the 3d Cartesian grid and aligned with the lightning observations
                (green
                symbols)
            </div>
        </div>



        <h2>Radar interpolation</h2>
        <p>The radar data are originally stored in range-azimuth-elevation space, which are spherical coordinates. These
            measurements must
            be mapped onto the previously defined common grid to align them with the lightning observations. The figure
            below illustrates the problem: The V-shaped structures are the cross-sections of the 12 radar cones, the
            black
            dots represent the grid points, with a horizontal resolution of 8 km and a vertical resolution of 0.5 km.
            These hollow cones now need to be converted into values on the grid, which is done through an anisotropic
            interpolation method.
        </p>

        <div class="figure">
            <img src="images/radar_cones_cross-section.png" class="medium-img" alt="cross-section of radar cone">
            <div class="caption">
                Cross-section of the 12 radar elevation cones (V-shapes) overlaid with the common 3D grid used for the
                interpolation.
            </div>
        </div>

        <p>Since the vertical resolution is far finer than the horizontal resolution, an anisotropic k-nearest-neighbor
            (kNN)
            approach is used. To avoid mixing information from different altitude layers, a pill-shaped neighborhood is
            constructed around each grid point by scaling the vertical coordinate to match the horizontal spacing. All
            radar data points within this rescaled volume are aggregated, and both the mean and maximum reflectivity
            values are computed. Afterwards, the volume is returned to its original proportions. The image below shows
            the result of this interpolation method. </p>

        <div class="figure">
            <img src="images/altitudes_knn_anisotropic.png" class="medium-img" alt="interpolation results">
            <div class="caption">
                Interpolated radar reflectivity at all 20 altitude levels, from 200 m to 9.7 km.
            </div>
        </div>

        <p>At low altitudes, the radar beams intersect the atmosphere densely, so most grid cells have enough
            measurements to provide reliable statistics. Higher up, the radar cones become hollow and only intersect the
            grid in a few places. As a result, many high-altitude grid cells are empty as there are too few or no data
            points available. </p>


        <h2>Temporal strategy</h2>
        The image below illustrates the temporal strategy behind the machine-learning setup. The green arrow marks the
        reference time, or "now", which is the timestamp that links a radar-lightning pair. The idea is to use the most
        recent radar measurements, those produced in the past 10 minutes, to predict whether lightning will occur at a
        future time. The delay between the reference time and the predicted lightning map is called the <em>lead
            time</em>, and it is a free parameter of the algorithm. A second adjustable parameter is the <em>time
            window</em>, which defines the duration of the future interval in which lightning is aggregated. Taken
        together, the lead time and time window specify exactly which future lightning the model is asked to forecast.

        <div class="figure">
            <img src="images/temporal_strategy.png" class="large-img" alt="temporal strategy">
            <div class="caption">
                Temporal relationship between radar input and future lightning target.
            </div>
        </div>


        <h2>Feature-Target construction</h2>

        <h2>Temporal grouping</h2>

        <h2>Train/Val/Test splits</h2>

        <h2>Class imbalance & binary setup</h2>

        <h2>Models</h2>
        <h3>CNN</h3>
        <h3>ConvLSTM 2D</h3>
        <h3>ConvLSTM 3D</h3>

        <h2>Results</h2>
        <h3>Main metrics</h3>
        <h3>GIF (visual agreement)</h3>

        <h2>Discussion</h2>

        <h2>Next steps</h2>

        <h2>Conclusion</h2>

        <h2>Code</h2>

        <h2>Acknowledgements</h2>
        <p>Huge thanks to the Meteorological Institute.</p>

        <!-- Footer -->
        <div class=" footer">
            <div class="contact">
                <a href="mailto:harald.thommesen@outlook.com">Email</a>
                <a href="https://www.linkedin.com/in/h-thommesen/">LinkedIn</a>
                <a href="https://github.com/haraltho/lightning-nowcasting-deep-learning">GitHub</a>
            </div>
        </div>
    </div>
</body>

</html>